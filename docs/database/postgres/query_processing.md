# Query Processing

## Overview

The query processing contains following stages:

1. Parser: The parser generates a parse tree from an SQL statement in plain text.
2. Analyzer/Analyser: The analyzer/analyser carries out a semantic analysis of a parse tree and generates a query tree.
3. Rewriter: The rewriter transforms a query tree using the rules stored in the rule system if such rules exist.
4. Planner: The planner generates the plan tree that can most effectively be executed from the query tree.
5. Executor: The executor executes the query via accessing the tables and indexes in the order that was created by the plan tree.

![](https://user-images.githubusercontent.com/17776979/200611573-3493ef3a-60c8-4894-afbd-6e9c09f5ea29.png)

### Parser

The parser generates a parse tree that can be read by subsequent subsystems from an SQL statement in plain text.

![](https://user-images.githubusercontent.com/17776979/200611779-788c8311-bc07-488c-8926-6e1708674d1f.png)

The parser only checks the syntax of an input when generating a parse tree, it only returns an error if there is a syntax error in the query. The parser does not check the semantics of an input query. For example, even if the query contains a table name that does not exist, the parser does not return an error. Semantic checks are done by the analyzer/analyser.

### Analyzer/Analyser

The analyzer/analyser runs a semantic analysis of a parse tree generated by the parser and generates a query tree.

![](https://user-images.githubusercontent.com/17776979/200612222-7c790dec-282e-40fd-9e87-ca6e65e077ce.png)

### Rewriter

The rewriter transforms a query tree according to the rules stored in the `pg_rules` system catalog if necessary

Find more information about rule system [here](https://www.sqlservercentral.com/articles/rules-in-postgresql)

### Planner and Executor

The planner receives a query tree from the rewriter and generates a (query) plan tree that can be processed by the executor most effectively.

A plan tree is composed of elements called plan nodes, and it is connected to the plantree list of the PlannedStmt structure. Each plan node has information that the executor requires for processing, and the executor processes from the end of the plan tree to the root in the case of a single-table query.

![](https://user-images.githubusercontent.com/17776979/200614885-c14c8451-b6e0-4101-9945-067b004dd0e3.png)

## Cost Estimation

PostgreSQL's query optimization is based on cost.

In PostgreSQL, there are three kinds of costs: **start-up**, **run** and **total**. The **total** cost is the sum of start-up and run costs; thus, only the start-up and run costs are independently estimated.

- The **start-up** cost is the cost expended before the first tuple is fetched. For example, the start-up cost of the index scan node is the cost to read index pages to access the first tuple in the target table. For a sequential scan, the startup cost will generally be close to zero, as it can start fetching rows straight away. For a sort operation, it will be higher because a large proportion of the work needs to be done before rows can start being returned.
- The **run** cost is the cost to fetch all tuples.
- The **total** cost is the sum of the costs of both start-up and run costs.

The costs are in an **arbitrary** unit. A common misunderstanding is that they are in milliseconds or some other unit of time, but that’s not the case.

The cost units are anchored (by default) to a single sequential page read costing 1.0 units (seq_page_cost). Each row processed adds 0.01 (cpu_tuple_cost), and each non-sequential page read adds 4.0 (random_page_cost).
### How the costs are calculated

In order to calculate these costs, the Postgres query planner uses both constants(cpu_tuple_cost(0.01), cpu_operator_cost(0.0025), random_page_cost(4), seq_page_cost(4)) and metadata about the contents of the database. The metadata is often referred to as `statistics`.

These statistics include a number of very useful things, like roughly the number of rows each table has, and what the most common values in each column are.

## Creating the Plan Tree
The planner in PostgreSQL performs three steps, as shown below:

1. Preprocessing (normalize boolean expression, flattening AND/OR expression...)
2. Get the cheapest access path by estimating the costs of all possible access paths.
3. Create the plan tree from the cheapest path

## Join Operations

PostgreSQL supports three join operations: nested loop join, merge join and hash join.

### Nested Loop Join

This is the simplest and most general join strategy of all. PostgreSQL scans the outer relation sequentially, and for each result row it scans the inner relation for matching rows.

![](https://user-images.githubusercontent.com/17776979/200876451-13211a2f-6fa7-46ac-9928-56f693c6a10c.png) 

**Indexes that can help with nested loop joins**

Since we scan the outer relation sequentially, no index on the outer relation will help. But an index on the join key of the inner relation can speed up a nested loop join considerably.

**Use cases for the nested loop join strategy**

Nested loop joins are particularly efficient if the outer relation is small, because then the inner loop won’t be executed too often. If the outer relation is large, nested loop joins are usually very inefficient, even if they are supported by an index on the inner relation. Nested loop joins are also used as the only option if the join condition does not use the = operator.

### Merge Join

In a merge join, PostgreSQL picks all join conditions with the = operator. It then sorts both tables by the join keys (which means that the data types must be sortable). Then it iterates through both sorted lists and finds matching entries.

![](https://user-images.githubusercontent.com/17776979/200876509-43f9982f-25d5-4dac-b50f-bb6cd7cac1cc.png) 

**Indexes that help with a merge join**

An index on the sort keys can speed up sorting, so an index on the join keys on both relations can speed up a merge join.

**Use cases for the merge join strategy**

The optimizer usually chooses a merge join if the involved relations are both too big for a hash that fits into `work_mem`. So this is the best strategy for joining really large tables.
Merge join is only feasible if there is at least one join condition with the = operator.

### Hash Join

First, PostgreSQL scans the inner relation sequentially and builds a hash table, where the hash key consists of all join keys that use the = operator. Then it scans the outer relation sequentially and probes the hash for each row found to find matching join keys.

![](https://user-images.githubusercontent.com/17776979/200876534-0bd008f6-b6b0-437a-a464-ee89bf70e9f0.png) 

**Indexes that can help with hash joins**

Nope

**Use cases for the hash join strategy**

Hash joins are best if none of the involved relations are small, but the hash table for the smaller table fits in work_mem. This is because otherwise PostgreSQL would build the hash in several batches and store them in temporary disk files, which hurts performance.

Looking up values in a hash table only works if the operator in the join condition is =, so you need at least one join condition with that operator.

### How to make PostgreSQL choose the correct join strategy

You can disable different join strategies temporarily with the SET command, which changes a parameter in your current database session:

```
SET enable_hashjoin = off;
SET enable_mergejoin = off;
SET enable_nestloop = off;
```
